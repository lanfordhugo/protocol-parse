---
name: python-test-review
description: AI Python 测试代码质量审查专家,确保测试能够准确、可靠地验证业务需求。在以下场景使用: (1) 审查测试代码的需求覆盖与场景有效性 (2) 检查测试结构与可读性 (3) 评估断言质量 (4) 验证依赖隔离与 Mock/Stub 使用 (5) 检查 FIRST 原则符合性 (6) 审查 pytest 框架使用规范 (7) 确保测试遵循 Python 测试生态系统最佳实践
---


# AI Python测试代码质量检查清单

## 核心审查原则

本文件定义了AI进行**Python测试代码**审查时 **必须** 遵循的客观标准和检查清单。

- **核心目标:** 确保测试能够**准确、可靠地验证业务需求**，同时保证测试代码本身是**清晰、可维护、值得信赖的**。
- **审查视角:** AI应以一个**经验丰富的Python测试开发者**视角进行审查，重点关注Python测试生态系统的最佳实践。
- **规则来源:** 本文档是所有Python测试代码评审活动的**唯一**标准来源。所有检查项都必须严格遵守。
- **冲突处理:** 当规则冲突时，优先级为：业务需求覆盖 > 测试真实性 > 代码质量 > 执行效率
- **测试框架:** 主要针对unittest、pytest、nose2等Python主流测试框架

---

## AI 测试代码质量检查清单

AI **必须** 逐项检查，给出"✓/✗/N/A"判断。对"✗"项提供具体改进建议和代码示例。连续3个"✗"时暂停并重新分析需求。

### 一、 需求覆盖与场景有效性

- **A. 功能需求覆盖:**
  - [ ] 测试用例是否清晰地映射并 **完全覆盖** 了相关需求文档中的所有功能点？
  - [ ] 是否覆盖了函数/方法的所有公共接口？
  - [ ] 是否测试了类的所有公共方法和属性？
- **B. 场景选择与代表性:**
  - [ ] 是否测试了最有代表性的"快乐路径"（正常流程）？
  - [ ] 是否充分覆盖了所有可预见的"异常路径"和错误处理分支？
  - [ ] 是否包含了对关键"边界条件"的测试（如None、0、空列表/字典、最大/最小值）？
  - [ ] 是否测试了Python特有的边界情况（如空字符串的布尔值、列表越界等）？
- **C. 测试数据有效性:**
  - [ ] 测试数据的选择是否恰当，能否有效触发被测行为？
  - [ ] 是否使用了有代表性的真实数据而非过于简化的测试数据？
  - [ ] 是否考虑了不同数据类型的组合测试？
- **D. 覆盖率量化标准:**
  - [ ] 核心业务逻辑代码覆盖率是否达到90%以上？
  - [ ] 分支覆盖率是否达到80%以上？
  - [ ] 是否使用了coverage.py等工具来监控覆盖率？
  - [ ] 是否排除了不需要测试的代码（如__init__.py、配置文件等）？

---

### 二、 测试结构与可读性

- **A. 结构清晰度 (AAA模式):**
  - [ ] 每个测试的结构是否严格遵循了 **AAA (Arrange-Act-Assert)** 模式？
  - [ ] 测试方法的命名是否清晰地描述了其意图（如 `test_function_name_when_condition_then_expected_result`）？
  - [ ] 是否使用了描述性的测试类名（如 `TestUserAuthentication`）？
- **B. 代码简洁度:**
  - [ ] 测试逻辑是否简单明了，是否避免了在测试代码中使用复杂的控制流（如if/else, for循环）？
  - [ ] 是否避免了使用"魔法数字"或硬编码字符串，而是使用了具名常量？
  - [ ] 每个测试方法是否只测试一个具体的行为或场景？
  - [ ] 测试代码是否比被测代码更简单易懂？
- **C. 注释质量:**
  - [ ] 对于复杂的测试设置(Arrange)或断言(Assert)逻辑，是否有清晰的注释解释其"为什么"？
  - [ ] 测试用例是否有文档字符串说明测试目的？
  - [ ] 复杂的测试数据构造是否有注释说明？
- **D. 规范符合性:**
  - [ ] 代码是否 **完全** 遵循了PEP 8编码规范？
  - [ ] 测试文件命名是否遵循约定（test_*.py或*_test.py）？
  - [ ] 测试方法命名是否以test_开头？
  - [ ] 是否正确使用了unittest.TestCase或pytest的测试结构？

---

### 三、 断言质量 (Assertion Quality)

- **A. 断言精确性:**
  - [ ] 断言是否只验证了当前测试场景应该变化的**最小化**结果（状态或返回值），避免了脆弱的大而全的断言？
  - [ ] 每个测试是否只包含最少数量的、必要的断言？（通常一个测试只验证一个核心关注点）
  - [ ] 是否使用了最具体的断言方法（如assertEqual而非assertTrue）？
- **B. 断言表达力:**
  - [ ] 断言失败时的消息是否清晰、明确？（是否使用了自定义错误消息）？
  - [ ] 是否使用了合适的断言方法（assertEqual、assertIn、assertRaises等）？
  - [ ] 是否避免了在断言中进行逻辑计算？
  - [ ] 对于复杂对象比较，是否使用了合适的断言方法（如assertDictEqual、assertListEqual）？
- **C. 真实性验证:**
  - [ ] 测试是否会在被测代码出错时失败？（通过故意引入缺陷验证）
  - [ ] 断言是否验证了实际业务行为而非代码执行？
  - [ ] 是否避免了过于宽泛的断言（如只检查返回值不为None）？
- **D. 异常测试:**
  - [ ] 对于应该抛出异常的场景，是否使用了assertRaises或pytest.raises？
  - [ ] 是否验证了异常的类型和消息内容？
  - [ ] 是否测试了异常处理的完整流程？

---

### 四、 依赖隔离与 Mock/Stub 使用

- **A. 依赖隔离:**
  - [ ] 被测单元(SUT)的外部依赖（IO、网络、数据库、其他类）是否都已被恰当地隔离（通过Mock/Stub）？
  - [ ] 是否使用了unittest.mock或pytest-mock来创建模拟对象？
  - [ ] 是否避免了在单元测试中访问真实的外部资源？
- **B. Mock/Stub 使用恰当性:**
  - [ ] 是否**优先进行状态验证**（检查SUT的结果），而非行为验证（检查SUT是否调用了某个mock方法）？
  - [ ] 当必须进行行为验证时，验证的交互是否是该测试核心关注的、必要的交互？
  - [ ] 是否避免了对内部实现细节或私有方法的Mock？
  - [ ] Mock对象的行为是否与真实对象保持一致？
- **C. Mock 框架使用规范:**
  - [ ] 是否正确使用了@patch装饰器或with语句来管理Mock的生命周期？
  - [ ] 是否使用了MagicMock、Mock、create_autospec等合适的Mock类型？
  - [ ] Mock的配置是否清晰明了（return_value、side_effect等）？
  - [ ] 是否在测试结束后正确清理了Mock状态？
- **D. 测试替身选择:**
  - [ ] 是否根据需要选择了合适的测试替身类型（Dummy、Fake、Stub、Mock、Spy）？
  - [ ] 对于复杂的外部依赖，是否考虑使用Fake对象而非Mock？
  - [ ] 是否避免了过度使用Mock导致测试与实现过度耦合？

---

### 五、 FIRST 原则符合性

- **A. 快速 (Fast):**
  - [ ] 测试运行是否足够快，不会拖慢开发反馈循环？
  - [ ] 是否避免了在单元测试中进行耗时的操作（如网络请求、文件IO、数据库操作）？
  - [ ] 是否使用了合适的测试运行器（如pytest的并行执行）来提高速度？
- **B. 独立 (Independent):**
  - [ ] 每个测试是否可以独立于其他任何测试，并以任意顺序运行？
  - [ ] `setUp`/`tearDown` (或pytest的fixture) 是否正确地创建和销毁了每个测试的独立环境？
  - [ ] 是否避免了测试之间的数据共享和状态依赖？
  - [ ] 是否正确使用了pytest的fixture作用域（function、class、module、session）？
- **C. 可重复 (Repeatable):**
  - [ ] 测试是否能在任何符合要求的环境中稳定地重复成功？
  - [ ] 是否依赖了不稳定的外部因素（如当前时间、随机数、网络状态、文件系统状态）？
  - [ ] 对于涉及时间的测试，是否使用了时间Mock或固定时间？
  - [ ] 对于涉及随机性的测试，是否固定了随机种子？
- **D. 自验证 (Self-Validating):**
  - [ ] 测试的输出是否是布尔型的（通过或失败），无需人工解读日志或输出？
  - [ ] 断言是否足够明确，失败时能清楚指出问题所在？
  - [ ] 是否避免了需要人工检查控制台输出的测试？
- **E. 及时 (Timely):**
  - [ ] 测试是否与产品代码一同编写和提交？
  - [ ] 是否遵循了TDD（测试驱动开发）或至少是测试优先的开发方式？
  - [ ] 新功能是否都有对应的测试覆盖？

---

---

## Python测试特定检查项

### 六、 Python测试生态系统最佳实践

- **A. 测试框架使用:**
  - [ ] 是否选择了合适的测试框架（unittest、pytest、nose2）？
  - [ ] 是否充分利用了所选框架的特性（如pytest的fixture、参数化测试）？
  - [ ] 是否正确使用了测试发现机制？
- **B. 参数化测试:**
  - [ ] 对于多个输入的相同逻辑，是否使用了参数化测试（@pytest.mark.parametrize）？
  - [ ] 参数化测试的参数是否有清晰的标识（ids参数）？
- **C. 测试数据管理:**
  - [ ] 是否使用了合适的测试数据管理方式（fixture、factory、builder模式）？
  - [ ] 大型测试数据是否存储在单独的文件中？
  - [ ] 是否避免了在测试代码中硬编码大量测试数据？
- **D. 测试配置:**
  - [ ] 是否有合适的测试配置文件（pytest.ini、tox.ini、setup.cfg）？
  - [ ] 测试环境是否与生产环境隔离？
  - [ ] 是否配置了合适的测试覆盖率报告？

---

## AI快速自检清单

执行Python测试审查前，AI必须确认：
- [ ] 已理解相关需求文档的业务需求
- [ ] 已分析被测Python模块的核心功能和接口
- [ ] 已识别Python特有的边界条件和异常场景
- [ ] 已确认测试框架和工具的正确使用
- [ ] 测试能回答："这保护了什么业务价值？"
- [ ] 测试是否遵循了Python社区的最佳实践

